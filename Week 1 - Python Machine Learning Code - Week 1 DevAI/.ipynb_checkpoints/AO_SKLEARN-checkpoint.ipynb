{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Oxford_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyWa9zaxY5oI"
   },
   "source": [
    "# `scikit-learn` Tutorial\n",
    "* This notebook covers different code implements from the scikit learn library, including:\n",
    "  * Recaling and Standardizing Data \n",
    "  * Model Evaluation and Metrics \n",
    "  * Classification and regression machine learning algorithms \n",
    "  * Regularization\n",
    "  * Optimising parameters \n",
    "  * Ensemble models \n",
    "  * Building Pipelines\n",
    "* We will use classification and regression datasets from `scikit-learn`, and will take code from the *Python Machine Learning* coursebook, as well as [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.co.uk/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=sr_1_1?ie=UTF8&qid=1541011829&sr=8-1&keywords=hands+on+machine+learning+with+scikit-learn+and+tensorflow) and [Machine Learning with Python Cookbook](https://www.amazon.co.uk/Machine-Learning-Python-Cookbook-Chris/dp/1491989386/ref=pd_sim_14_5?_encoding=UTF8&pd_rd_i=1491989386&pd_rd_r=e03d308c-dd3d-11e8-9244-bbcc5e42676d&pd_rd_w=lQiym&pd_rd_wg=lVZwd&pf_rd_i=desktop-dp-sims&pf_rd_m=A3P5ROKL5A1OLE&pf_rd_p=1e3b4162-429b-4ea8-80b8-75d978d3d89e&pf_rd_r=KBZG3157A75PS5MPGY8K&pf_rd_s=desktop-dp-sims&pf_rd_t=40701&psc=1&refRID=KBZG3157A75PS5MPGY8K) (both excellent books should you wish to continue your studies in this area).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Approach and Exercises\n",
    "* Note that this notebook is not focussed on how to structure an end to end machine learning problem, but is about extending the code implementations you will see in the end-to-end tutorials to further your knowledge.\n",
    "* This notebook is a progression from the previous notebooks, meaning that the exercises will generally be more involved. We will provide code implementations for one of the datasets and leave space for you to code implementations on a different dataset. Support is available via slack. \n",
    "* Each section will have an exercise to help reinforce your learning. We suggest you:\n",
    "   * Write out each line of code by hand (rather than copy and paste it from the relevant example) - this will improve your understanding of code syntax\n",
    "   * Write out, above each line of code, an explanation as to what the code, using a comment `#` - this will improve your understanding of how the code works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KA2bg4ruhi0G"
   },
   "source": [
    "## 1. Tutorial set up \n",
    "* Import statements\n",
    "* Loading the datasets\n",
    "* Reviewing the ML problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VrrYp-SY2Z4"
   },
   "outputs": [],
   "source": [
    "# import statements \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we will generally import scikit learn libraries when the are required, so you understand which specifc libraries are required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A19qwVSK8oRL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# load the classification dataset\n",
    "class_data = load_breast_cancer()\n",
    "class_X = pd.DataFrame(class_data.data, columns = class_data.feature_names)\n",
    "class_y = class_data.target\n",
    "\n",
    "# check the data has loaded successfully \n",
    "print(class_X.shape)\n",
    "print(class_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK67KgsUDAvk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# load the regression dataset\n",
    "boston = load_boston()\n",
    "boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "boston_y = boston.target\n",
    "\n",
    "# check the data has loaded successfully \n",
    "print(boston_X.shape)\n",
    "print(boston_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Em1bucYpgKsS"
   },
   "source": [
    "## 2. Rescaling and Standardizing Data \n",
    "* Machine Learning algorithms can receive perform benefits from data that has been rescaled and standardized, either in terms of speed or accuracy, or both.\n",
    "* There are a number of different ways of doing this, and implementations may vary because of algorithm requirements or personal preference.\n",
    "* We cover two common implementations below, and an additional one in the exercise. We'd encourage further research to understand the maths in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2dB0Y6qlAq5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features prior to rescaling:\n",
      "\n",
      "0    0.9053\n",
      "1    0.7339\n",
      "2    0.7869\n",
      "3    1.1560\n",
      "4    0.7813\n",
      "5    0.8902\n",
      "6    0.7732\n",
      "7    1.3770\n",
      "8    1.0020\n",
      "9    1.5990\n",
      "Name: texture error, dtype: float64\n",
      "\n",
      "Feature shape prior to reshaping:\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "## rescaling using MinMaxScaler, which converts data to a specified range, usually (0,1) or (-1,1)\n",
    "## many machine learning algorithms assume data is on the same scale  \n",
    "\n",
    "# here we import the necessary library \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create feature - we will use one feature but you may just pass all the features to rescaler \n",
    "feature = class_X['texture error']\n",
    "\n",
    "# print feature prior to rescaling\n",
    "print(\"Features prior to rescaling:\\n\")\n",
    "print(feature[0:10])\n",
    "\n",
    "# we will print the shape out as well \n",
    "print(\"\\nFeature shape prior to reshaping:\")\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Puz03eB8SCDw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature shape after reshaping:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## because we have used a pandas column, our data has the wrong shape for the function\n",
    "## we need a 2D tuple\n",
    "\n",
    "# this will reshape the data \n",
    "feature = feature.values.reshape(-1,1)\n",
    "\n",
    "# notice we now have a (samples,feature) tuple not just a (samples,) tuple\n",
    "print(\"\\nFeature shape after reshaping:\")\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3VSoI24RvCf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after rescaling:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12046941],\n",
       "       [0.08258929],\n",
       "       [0.09430251],\n",
       "       [0.17587518],\n",
       "       [0.09306489],\n",
       "       [0.11713225],\n",
       "       [0.09127475],\n",
       "       [0.22471711],\n",
       "       [0.14184052],\n",
       "       [0.27378006]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to instantiate the scaler and pass in the range we want our values to be scaled within \n",
    "minmax_scale = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the feature - note the fit_transform function, in the next example we will separate these steps\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "# lets look at the features now we have rescaled\n",
    "print(\"Features after rescaling:\\n\")\n",
    "scaled_feature[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcILmN5JlA5W"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mean         std\n",
       "mean radius              14.127292    3.524049\n",
       "mean texture             19.289649    4.301036\n",
       "mean perimeter           91.969033   24.298981\n",
       "mean area               654.889104  351.914129\n",
       "mean smoothness           0.096360    0.014064\n",
       "mean compactness          0.104341    0.052813\n",
       "mean concavity            0.088799    0.079720\n",
       "mean concave points       0.048919    0.038803\n",
       "mean symmetry             0.181162    0.027414\n",
       "mean fractal dimension    0.062798    0.007060"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardizing rescales features so that they are normally distributed, with a mean of 0 and a std deviation of 1\n",
    "\n",
    "# import the appropriate library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we will transform all the features in the dataset, so let's look at it  prior to transformation\n",
    "mean_std = pd.DataFrame(data={'mean':class_X.mean(), 'std':class_X.std()})\n",
    "\n",
    "# call the dataframe\n",
    "mean_std[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khSirtDXVRAs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00,\n",
       "         9.84374905e-01,  1.56846633e+00,  3.28351467e+00,\n",
       "         2.65287398e+00,  2.53247522e+00,  2.21751501e+00,\n",
       "         2.25574689e+00,  2.48973393e+00, -5.65265059e-01,\n",
       "         2.83303087e+00,  2.48757756e+00, -2.14001647e-01,\n",
       "         1.31686157e+00,  7.24026158e-01,  6.60819941e-01,\n",
       "         1.14875667e+00,  9.07083081e-01,  1.88668963e+00,\n",
       "        -1.35929347e+00,  2.30360062e+00,  2.00123749e+00,\n",
       "         1.30768627e+00,  2.61666502e+00,  2.10952635e+00,\n",
       "         2.29607613e+00,  2.75062224e+00,  1.93701461e+00],\n",
       "       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00,\n",
       "         1.90870825e+00, -8.26962447e-01, -4.87071673e-01,\n",
       "        -2.38458552e-02,  5.48144156e-01,  1.39236330e-03,\n",
       "        -8.68652457e-01,  4.99254601e-01, -8.76243603e-01,\n",
       "         2.63326966e-01,  7.42401948e-01, -6.05350847e-01,\n",
       "        -6.92926270e-01, -4.40780058e-01,  2.60162067e-01,\n",
       "        -8.05450380e-01, -9.94437403e-02,  1.80592744e+00,\n",
       "        -3.69203222e-01,  1.53512599e+00,  1.89048899e+00,\n",
       "        -3.75611957e-01, -4.30444219e-01, -1.46748968e-01,\n",
       "         1.08708430e+00, -2.43889668e-01,  2.81189987e-01],\n",
       "       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00,\n",
       "         1.55888363e+00,  9.42210440e-01,  1.05292554e+00,\n",
       "         1.36347845e+00,  2.03723076e+00,  9.39684817e-01,\n",
       "        -3.98007910e-01,  1.22867595e+00, -7.80083377e-01,\n",
       "         8.50928301e-01,  1.18133606e+00, -2.97005012e-01,\n",
       "         8.14973504e-01,  2.13076435e-01,  1.42482747e+00,\n",
       "         2.37035535e-01,  2.93559404e-01,  1.51187025e+00,\n",
       "        -2.39743838e-02,  1.34747521e+00,  1.45628455e+00,\n",
       "         5.27407405e-01,  1.08293217e+00,  8.54973944e-01,\n",
       "         1.95500035e+00,  1.15225500e+00,  2.01391209e-01],\n",
       "       [-7.68909287e-01,  2.53732112e-01, -5.92687167e-01,\n",
       "        -7.64463792e-01,  3.28355348e+00,  3.40290899e+00,\n",
       "         1.91589718e+00,  1.45170736e+00,  2.86738293e+00,\n",
       "         4.91091929e+00,  3.26373441e-01, -1.10409044e-01,\n",
       "         2.86593405e-01, -2.88378148e-01,  6.89701660e-01,\n",
       "         2.74428041e+00,  8.19518384e-01,  1.11500701e+00,\n",
       "         4.73268037e+00,  2.04751088e+00, -2.81464464e-01,\n",
       "         1.33984094e-01, -2.49939304e-01, -5.50021228e-01,\n",
       "         3.39427470e+00,  3.89339743e+00,  1.98958826e+00,\n",
       "         2.17578601e+00,  6.04604135e+00,  4.93501034e+00],\n",
       "       [ 1.75029663e+00, -1.15181643e+00,  1.77657315e+00,\n",
       "         1.82622928e+00,  2.80371830e-01,  5.39340452e-01,\n",
       "         1.37101143e+00,  1.42849277e+00, -9.56046689e-03,\n",
       "        -5.62449981e-01,  1.27054278e+00, -7.90243702e-01,\n",
       "         1.27318941e+00,  1.19035676e+00,  1.48306716e+00,\n",
       "        -4.85198799e-02,  8.28470780e-01,  1.14420474e+00,\n",
       "        -3.61092272e-01,  4.99328134e-01,  1.29857524e+00,\n",
       "        -1.46677038e+00,  1.33853946e+00,  1.22072425e+00,\n",
       "         2.20556166e-01, -3.13394511e-01,  6.13178758e-01,\n",
       "         7.29259257e-01, -8.68352984e-01, -3.97099619e-01],\n",
       "       [-4.76374665e-01, -8.35335303e-01, -3.87148067e-01,\n",
       "        -5.05650454e-01,  2.23742148e+00,  1.24433549e+00,\n",
       "         8.66301596e-01,  8.24655646e-01,  1.00540180e+00,\n",
       "         1.89000504e+00, -2.55070294e-01, -5.92661652e-01,\n",
       "        -3.21304185e-01, -2.89258217e-01,  1.56346702e-01,\n",
       "         4.45543649e-01,  1.60025198e-01, -6.91235537e-02,\n",
       "         1.34118807e-01,  4.86845840e-01, -1.65498247e-01,\n",
       "        -3.13836333e-01, -1.15009456e-01, -2.44320208e-01,\n",
       "         2.04851283e+00,  1.72161644e+00,  1.26324320e+00,\n",
       "         9.05887786e-01,  1.75406939e+00,  2.24180161e+00],\n",
       "       [ 1.17090767e+00,  1.60649427e-01,  1.13812505e+00,\n",
       "         1.09529491e+00, -1.23136226e-01,  8.82952423e-02,\n",
       "         3.00072399e-01,  6.46935108e-01, -6.43246179e-02,\n",
       "        -7.62332153e-01,  1.49883071e-01, -8.04939888e-01,\n",
       "         1.55410293e-01,  2.98627465e-01, -9.09029826e-01,\n",
       "        -6.51568010e-01, -3.10141387e-01, -2.28089026e-01,\n",
       "        -8.29666081e-01, -6.11217806e-01,  1.36898330e+00,\n",
       "         3.22882892e-01,  1.36832530e+00,  1.27521954e+00,\n",
       "         5.18640227e-01,  2.12149800e-02,  5.09552250e-01,\n",
       "         1.19671580e+00,  2.62475664e-01, -1.47304787e-02],\n",
       "       [-1.18516778e-01,  3.58450132e-01, -7.28668396e-02,\n",
       "        -2.18964911e-01,  1.60404905e+00,  1.14010235e+00,\n",
       "         6.10257495e-02,  2.81950258e-01,  1.40335463e+00,\n",
       "         1.66035318e+00,  6.43623001e-01,  2.90560957e-01,\n",
       "         4.90050986e-01,  2.33722421e-01,  5.88030871e-01,\n",
       "         2.68932704e-01, -2.32553954e-01,  4.35348506e-01,\n",
       "        -6.88004232e-01,  6.11668783e-01,  1.63762976e-01,\n",
       "         4.01047912e-01,  9.94485804e-02,  2.88594274e-02,\n",
       "         1.44796112e+00,  7.24785507e-01, -2.10538519e-02,\n",
       "         6.24195735e-01,  4.77640485e-01,  1.72643451e+00],\n",
       "       [-3.20166857e-01,  5.88829778e-01, -1.84080380e-01,\n",
       "        -3.84207273e-01,  2.20183876e+00,  1.68400981e+00,\n",
       "         1.21909628e+00,  1.15069158e+00,  1.96559991e+00,\n",
       "         1.57246173e+00, -3.56850016e-01, -3.89818004e-01,\n",
       "        -2.27743400e-01, -3.52403123e-01, -4.36677342e-01,\n",
       "         5.33290226e-01,  1.20568341e-01,  7.52430487e-02,\n",
       "         1.07481537e-01, -1.73631991e-02, -1.61356597e-01,\n",
       "         8.22813332e-01, -3.16091086e-02, -2.48363407e-01,\n",
       "         1.66275699e+00,  1.81830968e+00,  1.28003453e+00,\n",
       "         1.39161624e+00,  2.38985717e+00,  1.28864955e+00],\n",
       "       [-4.73534523e-01,  1.10543868e+00, -3.29481787e-01,\n",
       "        -5.09063378e-01,  1.58269942e+00,  2.56335845e+00,\n",
       "         1.73887209e+00,  9.41760326e-01,  7.97298024e-01,\n",
       "         2.78309559e+00, -3.88250143e-01,  6.93345302e-01,\n",
       "        -4.09419634e-01, -3.60763773e-01,  3.60084898e-02,\n",
       "         2.60958662e+00,  1.50984760e+00,  4.09394960e-01,\n",
       "        -3.21136366e-01,  2.37734605e+00, -2.44189609e-01,\n",
       "         2.44310906e+00, -2.86278027e-01, -2.97409172e-01,\n",
       "         2.32029536e+00,  5.11287727e+00,  3.99543285e+00,\n",
       "         1.62001520e+00,  2.37044380e+00,  6.84685604e+00]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler - this calculates the minimum and maximum values of the data\n",
    "scaler.fit(class_X)\n",
    "\n",
    "# transform the data using the fitted scaler - this applies the transform using the fit\n",
    "standardized = scaler.transform(class_X)\n",
    "\n",
    "# this shows the features transformed - note that this returns an array not a dataframe\n",
    "standardized[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIRzFhg0X_cA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>-6.118909e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mean  std\n",
       "mean radius            -6.118909e-16  1.0\n",
       "mean texture           -6.118909e-16  1.0\n",
       "mean perimeter         -6.118909e-16  1.0\n",
       "mean area              -6.118909e-16  1.0\n",
       "mean smoothness        -6.118909e-16  1.0\n",
       "mean compactness       -6.118909e-16  1.0\n",
       "mean concavity         -6.118909e-16  1.0\n",
       "mean concave points    -6.118909e-16  1.0\n",
       "mean symmetry          -6.118909e-16  1.0\n",
       "mean fractal dimension -6.118909e-16  1.0"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets have another look at our transformed data\n",
    "mean_std_transformed = pd.DataFrame(data={'mean':standardized.mean(), 'std':standardized.std()}, index=class_X.columns)\n",
    "\n",
    "# call the dataframe\n",
    "mean_std_transformed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WS2xLaapj-xV"
   },
   "source": [
    "### EXERCISE SECTION 2: \n",
    "* Apply the `Rescaling` and `Standardization` functions to one or more features of the `Boston` dataset\n",
    "* Research and implement a `Normalizer` function\n",
    "* Have a think about what circumstances each of these functions might be most appropriate to apply to different data and why \n",
    "* We can discuss the implementations and benefits of each approach on Slack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE CODE GOES HERE \n",
    "\n",
    "def scale_numeric(df, scaler):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    numeric_columns = [f for f in data.columns if data.dtypes[f] != 'object']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.413229</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>0.439316</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>-0.625796</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.387217</td>\n",
       "      <td>-0.418147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.415249</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>-0.234548</td>\n",
       "      <td>0.288933</td>\n",
       "      <td>-0.716639</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.500850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413447</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>0.984960</td>\n",
       "      <td>0.797449</td>\n",
       "      <td>-0.773684</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.983048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407764</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>0.725672</td>\n",
       "      <td>0.736996</td>\n",
       "      <td>-0.668437</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.403225</td>\n",
       "      <td>-0.865302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.415000</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>-0.362767</td>\n",
       "      <td>0.434732</td>\n",
       "      <td>-0.613246</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.669058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1   -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2   -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3   -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4   -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "501 -0.413229 -0.487722  0.115738 -0.272599  0.158124  0.439316  0.018673   \n",
       "502 -0.415249 -0.487722  0.115738 -0.272599  0.158124 -0.234548  0.288933   \n",
       "503 -0.413447 -0.487722  0.115738 -0.272599  0.158124  0.984960  0.797449   \n",
       "504 -0.407764 -0.487722  0.115738 -0.272599  0.158124  0.725672  0.736996   \n",
       "505 -0.415000 -0.487722  0.115738 -0.272599  0.158124 -0.362767  0.434732   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  \n",
       "1    0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439  \n",
       "2    0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  \n",
       "3    1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  \n",
       "4    1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "501 -0.625796 -0.982843 -0.803212  1.176466  0.387217 -0.418147  \n",
       "502 -0.716639 -0.982843 -0.803212  1.176466  0.441052 -0.500850  \n",
       "503 -0.773684 -0.982843 -0.803212  1.176466  0.441052 -0.983048  \n",
       "504 -0.668437 -0.982843 -0.803212  1.176466  0.403225 -0.865302  \n",
       "505 -0.613246 -0.982843 -0.803212  1.176466  0.441052 -0.669058  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "boston_X_rsc_ss = scale_numeric(boston_X, scaler)\n",
    "boston_X_rsc_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>0.641607</td>\n",
       "      <td>0.269203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>0.063466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.994276</td>\n",
       "      <td>0.033389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.681771</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.987619</td>\n",
       "      <td>0.219095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.760041</td>\n",
       "      <td>0.105293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.654340</td>\n",
       "      <td>0.907312</td>\n",
       "      <td>0.094381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.889804</td>\n",
       "      <td>0.114514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.991301</td>\n",
       "      <td>0.131071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.473079</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>0.125072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
       "0    0.000000  0.18  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
       "1    0.000236  0.00  0.242302   0.0  0.172840  0.547998  0.782698  0.348962   \n",
       "2    0.000236  0.00  0.242302   0.0  0.172840  0.694386  0.599382  0.348962   \n",
       "3    0.000293  0.00  0.063050   0.0  0.150206  0.658555  0.441813  0.448545   \n",
       "4    0.000705  0.00  0.063050   0.0  0.150206  0.687105  0.528321  0.448545   \n",
       "..        ...   ...       ...   ...       ...       ...       ...       ...   \n",
       "501  0.000633  0.00  0.420455   0.0  0.386831  0.580954  0.681771  0.122671   \n",
       "502  0.000438  0.00  0.420455   0.0  0.386831  0.490324  0.760041  0.105293   \n",
       "503  0.000612  0.00  0.420455   0.0  0.386831  0.654340  0.907312  0.094381   \n",
       "504  0.001161  0.00  0.420455   0.0  0.386831  0.619467  0.889804  0.114514   \n",
       "505  0.000462  0.00  0.420455   0.0  0.386831  0.473079  0.802266  0.125072   \n",
       "\n",
       "          RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    0.000000  0.208015  0.287234  1.000000  0.089680  \n",
       "1    0.043478  0.104962  0.553191  1.000000  0.204470  \n",
       "2    0.043478  0.104962  0.553191  0.989737  0.063466  \n",
       "3    0.086957  0.066794  0.648936  0.994276  0.033389  \n",
       "4    0.086957  0.066794  0.648936  1.000000  0.099338  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "501  0.000000  0.164122  0.893617  0.987619  0.219095  \n",
       "502  0.000000  0.164122  0.893617  1.000000  0.202815  \n",
       "503  0.000000  0.164122  0.893617  1.000000  0.107892  \n",
       "504  0.000000  0.164122  0.893617  0.991301  0.131071  \n",
       "505  0.000000  0.164122  0.893617  1.000000  0.169702  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "boston_X_rsc_mm = scale_numeric(boston_X, scaler)\n",
    "boston_X_rsc_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CRIM   ZN  INDUS  CHAS  NOX   RM  AGE  DIS  RAD  TAX  PTRATIO    B  LSTAT\n",
       "0     1.0  1.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "1     1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "2     1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "3     1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "4     1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "..    ...  ...    ...   ...  ...  ...  ...  ...  ...  ...      ...  ...    ...\n",
       "501   1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "502   1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "503   1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "504   1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "505   1.0  0.0    1.0   0.0  1.0  1.0  1.0  1.0  1.0  1.0      1.0  1.0    1.0\n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer(norm='l2')\n",
    "boston_X_rsc_n = scale_numeric(boston_X, normalizer)\n",
    "boston_X_rsc_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8E5I4vtgK7y"
   },
   "source": [
    "## 3. Model Evaluation and Metrics \n",
    "* We need to understand how we are going to assess our model's performance. This involves:\n",
    "  * Splitting our Training and Testing data appropriately \n",
    "  * Choosing an Evaluation Metric\n",
    "  * Developing a Baseline Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uq-9z9Hgnwia"
   },
   "outputs": [],
   "source": [
    "## testing and training splits \n",
    "\n",
    "# import statement \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this splits our data into a test and training set\n",
    "# the function shuffles our data before splitting (so will get both classes in both sets)\n",
    "# setting a random_state means that this shuffling will be consistent for each run \n",
    "# setting stratify=class_y tells the function to have an even number of class labels in each set\n",
    "X_train, X_test, y_train, y_test = train_test_split(class_X,class_y, test_size=0.3, random_state=1, stratify = class_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LcIe8dY_50z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [212 357]\n",
      "Percentage of class zeroes in class_y 37.0\n",
      "\n",
      "\n",
      "Labels counts in y_train: [148 250]\n",
      "Percentage of class zeroes in y_train 37.0\n",
      "\n",
      "\n",
      "Labels counts in y_test: [ 64 107]\n",
      "Percentage of class zeroes in y_test 37.0\n"
     ]
    }
   ],
   "source": [
    "# we can verify the stratifications using np.bincount\n",
    "print('Labels counts in y:', np.bincount(class_y))\n",
    "print('Percentage of class zeroes in class_y',np.round(np.bincount(class_y)[0]/len(class_y)*100))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Percentage of class zeroes in y_train',np.round(np.bincount(y_train)[0]/len(y_train)*100))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Labels counts in y_test:', np.bincount(y_test))\n",
    "print('Percentage of class zeroes in y_test',np.round(np.bincount(y_test)[0]/len(y_test)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEM_Ub4wCx7I"
   },
   "source": [
    "* Note that there are additional ways to divide our data into test and training sets, and we cover `K-fold cross validation` in the **Pipeline** section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLckno3Rnw-N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=1, strategy='uniform')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can create a baseline model to benchmark our other estimators against\n",
    "## this can be a simple estimator or we can use a dummy estimator to make predictions in a random manner \n",
    "\n",
    "# this is the required import statement \n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# this creates our dummy classifier, and the value we pass in to the strategy parameter dtermn\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrBocYv4nwwW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47953216374269003"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## evaluating the model metrics\n",
    "\n",
    "# Here we get an accuracy score\n",
    "dummy.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5vlaKNKRPeC"
   },
   "source": [
    "* There are a number of different ways in `scikit-learn` to get an estimator score and it can get confusing first.\n",
    "* Remember that to get a score, we need to instantiate a model, fit it to the data, predict using unseen data, compare the predictions against actual data, and score the difference. This is true for classification and regression problems, and is true no matter the method used to get there.\n",
    "  * So, in the end-to-end tutorials we split the training and test data,  fitted our data to an estimator, and called the `.predict` method on the estimator to get our predictions, and then passed this to a scoring function (four steps)\n",
    "  * In using the `estimator.score()`method above, we are passing in our split data and the method is then making predictions and returning the score (three steps). \n",
    "  * And, in the `cross_val_score()` method used below we are effectively using one step as the method takes an estimator and our data and returns a score. You can find out more about this method [here](https://scikit-learn.org/stable/modules/cross_validation.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEd1YkKQHHqo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98591549, 0.97183099, 0.98611111, 0.95833333, 0.95774648])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## here we fit a new estimator and use cross_val_score to get a score based on a defined metric \n",
    "\n",
    "# import statements\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate logistic regression classifier\n",
    "logistic = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# we pass our estimator and data to the method. we also specify the number of folds (default is 3)\n",
    "# the default scring method is the one associated with the estimator we pass in\n",
    "# we can use the scoring parameter to pass in different scoring methods. Here we use recall.  \n",
    "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Od2_2Tq1n0pc"
   },
   "source": [
    "### EXERCISE SECTION 3:\n",
    "* Part 1: Implement binary classification scoring functions \n",
    "  * Use `cross_val_score` to implement `f1`, and `precision` scores \n",
    "  * Understand what these scores, and the `accuracy` and `recall` scores implemented above, tell us\n",
    "* Part 2: Another way of assessing our model's performance is with the `Receiving Operating Characteristic (ROC) Curve`. \n",
    "  * What does this show us? Can you implement it?\n",
    "  * Like the other questions here, we can work through these via slack.\n",
    "* Part 3: What are the main ways of evaluating a mutliclass classification problem?\n",
    "  * Research and make notes below\n",
    "* Part 4: Apply the above approach to the Boston dataset. You should think about:\n",
    "  * What sort of test and training split you want to implement \n",
    "    * Be careful to change the variable names for `X_train`, `y_train` etc. so that the new dataset doesn't overwrite those variables\n",
    "    * Experiment with not using the `stratify =  ` variable\n",
    "  * The evaluation metrics you wish to use and why (this is different than above as the Boston dataset is a regression problem)\n",
    "    * Evaluation metrics are covered in *Python Machine Learning*, Chapter 6\n",
    "  * Implementing a baseline model \n",
    "    * *Python Machine Learning*, Chapter 10 has linear regression implementations to draw from.\n",
    "* Feel free to discuss your approach on Slack and ask for support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89mkhQL_HH3A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94594595, 0.95172414, 0.97931034, 0.95833333, 0.97142857])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EXERCISE SECTION 3: PART 1\n",
    "## use the cross_val_score function to pass in and return the scoring functions: precision, f1 \n",
    "## note the scores and research what each score represents \n",
    "\n",
    "# F1-score is a measure of a model’s accuracy on a dataset. It combines the precision and recall of the model, \n",
    "# and it is defined as the harmonic mean of both\n",
    "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.93243243, 0.97260274, 0.95833333, 0.98550725])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision is the fraction of true positive examples among the examples that the model classified as positive. \n",
    "# In other words, the number of true positives divided by the number of false positives plus true positives.\n",
    "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall is the fraction of examples classified as positive, among the total number of positive examples.\n",
    "# In other words, the number of true positives divided by the number of true positives plus false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99443171, 0.99279397, 0.99702381, 0.98280423, 0.99664655])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EXERCISE SECTION 3: PART 2\n",
    "## look up and implement the ROC metric \n",
    "\n",
    "# AUC-ROC curve is a performance measurement for classification problem at various thresholds settings. \n",
    "# ROC is a probability curve and AUC represents degree or measure of separability. \n",
    "# It tells how much model is capable of distinguishing between classes. \n",
    "# Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.\n",
    "\n",
    "cross_val_score(logistic, class_X, class_y, cv=5, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0vDRITZL1M1"
   },
   "source": [
    "**EXERCISE SECTION 3: PART 3**\n",
    "* What are the evaluation metrics that can be used for mutliclass regression problems?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Answer**\n",
    "* In the case of multiclass regression problems, the evaluation metrics used are the following:\n",
    "\n",
    "- Mean Absolute Error (MAE) - indicates the average sum of absolute difference between the actual and predicted value.\n",
    "\n",
    "- Mean Squared Error (MSE) - calculates the average sum of squared difference between the actual and predicted value for the entire data points. All related values are raised to the second power therefore all of negative values are not compensated by positives\n",
    "\n",
    "- Root Mean Squared Error (RMSE) - square root of MSE. It is easy to interpret compared to MSE and it uses smaller absolute values which is helpful for computer calculations.\n",
    "\n",
    "- R2 - represents the proportion of the variance for a dependent variable that's explained by an independent variable(s). In other words, it shows to what extent the variance of one variable explains the variance of the second variable. So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.\n",
    "\n",
    "- Mean Absolute Percentage Error (MAPE) – measures the accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Number of datapoints:  354\n",
      "Number of features:  13\n",
      "\n",
      "\n",
      "Test Set:\n",
      "Number of datapoints:  152\n",
      "Number of features:  13\n",
      "\n",
      "MSE:  19.831323672063235\n",
      "MAE:  3.3446655035987476\n",
      "R2 :  0.7836295385076281\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## EXERCISE SECTION 3: PART 4 \n",
    "## Implement a baseline model and evaluation metric for the Boston dataset (regression)\n",
    "\n",
    "# setting stratify=boston_y tells the function to have an even number of class labels in each set\n",
    "#dX_train, dX_test, dy_train, dy_test = train_test_split(boston_X,boston_y, test_size=0.3, random_state=1, stratify = boston_y)\n",
    "\n",
    "'''Stratify here leads to the following error caused by class imbalancing:\n",
    "ValueError: The least populated class in y has only 1 member, which is too few. \n",
    "The minimum number of groups for any class cannot be less than 2.'''\n",
    "\n",
    "# no stratify\n",
    "dX_train, dX_test, dy_train, dy_test = train_test_split(boston_X,boston_y, test_size=0.3, random_state=1)\n",
    "\n",
    "# get shape of test and training sets\n",
    "print('Training Set:')\n",
    "print('Number of datapoints: ', dX_train.shape[0])\n",
    "print('Number of features: ', dX_train.shape[1])\n",
    "print('\\n')\n",
    "print('Test Set:')\n",
    "print('Number of datapoints: ', dX_test.shape[0])\n",
    "print('Number of features: ', dX_test.shape[1])\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "def evaluate(Y_test, Y_pred):\n",
    "    \n",
    "    ''' The 3 evaluation metrics chosen are the MAE, MSE and R2. \n",
    "    The first ones provide an indication of the error of the model. \n",
    "    The last one is the percentage of the response variable variation that is explained by the regression model,\n",
    "    allowing to understand how well the model is fitting the data.'''\n",
    "    \n",
    "    # Calculate errors\n",
    "    mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "    msa = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "    r2 = metrics.r2_score(Y_test, Y_pred)\n",
    "\n",
    "    print(\"\\nMSE: \", mse)\n",
    "    print(\"MAE: \", msa)\n",
    "    print(\"R2 : \", r2)\n",
    "    \n",
    "    # Plot\n",
    "    plt.scatter(Y_test, Y_pred)\n",
    "    plt.xlabel(\"y\")\n",
    "    plt.ylabel(\"y_hat\")\n",
    "    plt.title(\"y vs y_hat\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Implementing a baseline model, as linear regression is a simple model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "lm.fit(dX_train, dy_train)\n",
    "\n",
    "# Predict\n",
    "dy_pred = lm.predict(dX_test)\n",
    "\n",
    "# Evaluate\n",
    "evaluate(dy_test, dy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptGBIAY4gLI9"
   },
   "source": [
    "## 4. Classification and regression machine learning algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgIyAS2_qjSV"
   },
   "source": [
    "### EXERCISE SECTION 4:\n",
    "* You already have an idea about how to fit and instatiate machine learning models.\n",
    "* There are a large number of machine learning approaches you can use, and this exercise is about understanding how a number of them perform on our datasets.\n",
    "* Research and instantiate as many algorithms as you wish in the code cell marked below. \n",
    "  * There are a number of examples in *Python Machine Learning*, for example `SVM`, `Decision Trees` and `Logistic Regression` in Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LogisticRegression(solver='liblinear')\n",
      "\n",
      "--- TRAIN ---\n",
      "\n",
      "MSE:  0.04271356783919598\n",
      "MAE:  0.04271356783919598\n",
      "R2 :  0.8171351351351351\n",
      "\n",
      "--- TEST ---\n",
      "Accuracy:  0.9473684210526315\n",
      "F1 score:  0.958139534883721\n",
      "ROC_AUC:  0.9422459112149533\n",
      "\n",
      " SVC()\n",
      "\n",
      "--- TRAIN ---\n",
      "\n",
      "MSE:  0.08040201005025126\n",
      "MAE:  0.08040201005025126\n",
      "R2 :  0.6557837837837839\n",
      "\n",
      "--- TEST ---\n",
      "Accuracy:  0.9239766081871345\n",
      "F1 score:  0.9411764705882353\n",
      "ROC_AUC:  0.907856308411215\n",
      "\n",
      " DecisionTreeClassifier()\n",
      "\n",
      "--- TRAIN ---\n",
      "\n",
      "MSE:  0.0\n",
      "MAE:  0.0\n",
      "R2 :  1.0\n",
      "\n",
      "--- TEST ---\n",
      "Accuracy:  0.9473684210526315\n",
      "F1 score:  0.9577464788732395\n",
      "ROC_AUC:  0.9453855140186915\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## EXERCISE CODE HERE \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(class_X,class_y, test_size=0.3, random_state=1, stratify = class_y)\n",
    "\n",
    "def evaluate_set(y_test, y_pred):\n",
    "    # this block of code returns all the metrics we are interested in \n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print (\"Accuracy: \", accuracy)\n",
    "    print ('F1 score: ', f1)\n",
    "    print ('ROC_AUC: ' , auc)\n",
    "\n",
    "\n",
    "def get_model(X_train, y_train, X_test, y_test, model):\n",
    "\n",
    "    # Fit\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print('\\n',model)\n",
    "    print('\\n--- TRAIN ---')\n",
    "    evaluate(y_train, y_pred_train)\n",
    "    \n",
    "    print('\\n--- TEST ---')\n",
    "    evaluate_set(y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "get_model(X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "get_model(X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "get_model(X_train, y_train, X_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIFg9b-7wXgX"
   },
   "source": [
    "## 5. Regularization \n",
    "* When we train our models, we might need to prevent against **over-** or **underfitting**. Overfitting is when our model performs well on the training data but does not generalise to unseen data (such as our test set). The model might be too complex, with the parameter weights probably too large and too aligned too closely to the patterns of the training data. The solution is to develop a means of reducing these weights during training, and this is done through a process called **regularization**.\n",
    "* Underfitting is when our model is too simple to capture patterns in either the training or test sets. Chapters 3 and 10 of *Python Machine Learning* cover this topic in more detail.\n",
    "* How we regularise the model depends on what estimator we are using. In most cases, we can penalise the increase in parameter weights as the model trains by inserting a regularization term to our model when we instantiate it. This is applicable to regression models, logistics regression and support vector classifiers (see below for implementations).  \n",
    "* For these estimators, the regularization term is commonly either **L1 Regularization**, where the weights are reduced by a user defined parameter multiplied by the absolute value of the weights (or coefficients); or **L2 Regularization**, where the weights are reduced by a user defined parameter multiplied by the squared sum of the weights.\n",
    "* For tree based models, such as Decision Trees and Random Forests, we can help prevent overfitting by adjusting parameters associated with the leaf and branch settings.\n",
    "* Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales.\n",
    "* We implement L1 and L2 Regularization for regression models in this section. In Section 6, we implement  `RandomSearchCV` (explained below) on a `Logistic Regression` estimator. The parameters that we change in that implementation are related to regularization ('`C`' and '`penalty`')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuSLgswxwX4w"
   },
   "outputs": [],
   "source": [
    "## For regularization in regression we instantiate new models\n",
    "## Lasso regression is used for L1 Regularization \n",
    " \n",
    "# import statement \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create lasso regression with alpha value - this is our hyperparameter \n",
    "regression = Lasso(alpha=0.5)\n",
    " \n",
    "# Fit the linear regression\n",
    "model_l1 = regression.fit(class_X, class_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LASLHhqSxvPt"
   },
   "outputs": [],
   "source": [
    "## Ridge regression is L2 Regularization\n",
    "\n",
    "# import statement \n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create ridge regression with an alpha value\n",
    "regression = Ridge(alpha=0.5)\n",
    " \n",
    "# Fit the linear regression\n",
    "model_l2 = regression.fit(class_X, class_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kWobSEgwqad"
   },
   "source": [
    "### EXERCISE FOR SECTION 5\n",
    "* Part 1: Train both the models above and get a score. Compare the results and see if one type of regularization works better than another.\n",
    "* Part 2: The `ElasticNet`estimator combines both L1 and L2 Regularization. Look up the estimator on the scikit-learn documentation and a) write out how it combines both regularization methods, and b) see if you can implement it.\n",
    "* Part 3: We can use `RidgeCV` and `LassoCV` to explore the best regularization parameters for each estimator. Implement one or both of these estimators, defining the parameters you want to search over.\n",
    "* Part 4: Look at the `DecisionTreeClassifier `documentation in `scikit-learn` and come to a view on which parameters can be set to help prevent overfitting (and therefore help regularization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JO1SGtIRwYHA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lasso(alpha=0.5)\n",
      "\n",
      "--- TRAIN ---\n",
      "Mean squared error:  26.241221939540807\n",
      "Mean absolute error:  3.5275389608106353\n",
      "R^2 :  0.6767887090980561\n",
      "\n",
      "--- TEST ---\n",
      "Mean squared error:  23.91888317593273\n",
      "Mean absolute error:  3.516290110040999\n",
      "R^2 :  0.7390320547059954\n",
      "\n",
      " Ridge(alpha=0.5)\n",
      "\n",
      "--- TRAIN ---\n",
      "Mean squared error:  23.65324777268542\n",
      "Mean absolute error:  3.30838551999744\n",
      "R^2 :  0.7086646054727526\n",
      "\n",
      "--- TEST ---\n",
      "Mean squared error:  19.42728523202775\n",
      "Mean absolute error:  3.2987471493270375\n",
      "R^2 :  0.7880378162997095\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 5: PART 1 \n",
    "\n",
    "def evaluate_reg(Y_test, Y_pred):\n",
    "    mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "    msa = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "    r2 = metrics.r2_score(Y_test, Y_pred)\n",
    "\n",
    "    print(\"Mean squared error: \", mse)\n",
    "    print(\"Mean absolute error: \", msa)\n",
    "    print(\"R^2 : \", r2)\n",
    "\n",
    "\n",
    "def get_model_reg(X_train, y_train, X_test, y_test, model):\n",
    "\n",
    "    # Fit\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print('\\n',model)\n",
    "    print('\\n--- TRAIN ---')\n",
    "    evaluate_reg(y_train, y_pred_train)\n",
    "    \n",
    "    print('\\n--- TEST ---')\n",
    "    evaluate_reg(y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "clf = Lasso(alpha=0.5)\n",
    "get_model_reg(dX_train, dy_train, dX_test, dy_test, clf)\n",
    "\n",
    "clf = Ridge(alpha=0.5)\n",
    "get_model_reg(dX_train, dy_train, dX_test, dy_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZAm2HeawYhd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ElasticNet(alpha=0.5)\n",
      "\n",
      "--- TRAIN ---\n",
      "Mean squared error:  26.45618117759185\n",
      "Mean absolute error:  3.5481651291167475\n",
      "R^2 :  0.6741410712334075\n",
      "\n",
      "--- TEST ---\n",
      "Mean squared error:  24.50940529586544\n",
      "Mean absolute error:  3.540721555469341\n",
      "R^2 :  0.7325891391586441\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 5: PART 2 \n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "'''ElasticNet is based on linear regression with combined L1 and L2 priors as regularizer.\n",
    "It minimizes the objective function:\n",
    "1 / (2 * n_samples) * ||y - Xw||^2_2\n",
    "+ alpha * l1_ratio * ||w||_1\n",
    "+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2'''\n",
    "\n",
    "clf = ElasticNet(alpha=0.5)\n",
    "get_model_reg(dX_train, dy_train, dX_test, dy_test, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "718i3izO3dEv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  0.1\n",
      "\n",
      " Ridge(alpha=0.1)\n",
      "\n",
      "--- TRAIN ---\n",
      "Mean squared error:  23.523241192251707\n",
      "Mean absolute error:  3.3322370986619863\n",
      "R^2 :  0.7102658874094145\n",
      "\n",
      "--- TEST ---\n",
      "Mean squared error:  19.696199831814127\n",
      "Mean absolute error:  3.326904927834307\n",
      "R^2 :  0.7851038126487186\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 5: PART 3\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "clf = RidgeCV(alphas=alphas, cv=5)\n",
    "\n",
    "mdl = clf.fit(dX_train, dy_train)\n",
    "\n",
    "best_params = mdl.alpha_\n",
    "print('Best alpha: ',best_params)\n",
    "\n",
    "clf = Ridge(alpha=best_params)\n",
    "get_model_reg(dX_train, dy_train, dX_test, dy_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0d_iE883eXP"
   },
   "source": [
    "#### EXERCISE SECTION 5: PART 4\n",
    "There are several approaches to avoiding overfitting in building decision trees: \t\t\n",
    "- Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.\n",
    "- Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree.\n",
    "\n",
    "Therefore in order to prevent overfitting we can tune the following parameters:\n",
    "\n",
    "- `ccp_alphanon`- Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed.\n",
    "\n",
    "- `min_impurity_decrease`-  A node will be split if this split induces a decrease of the impurity greater than or equal to this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrVZ5owOgLlh"
   },
   "source": [
    " ## 6. Optimizing parameters \n",
    " * We applied `GridSearchCV` to identify the best hyperparameters for our models in the in-class regression tutorials.\n",
    " * There are other methods available to use that don't take the brute force approach of `GridSearchCV`.\n",
    " * We will cover an implementation of `RandomizedSearchCV` below, and use the exercise for you to implement it on the other datatset.\n",
    "  * We use this method to search over defined hyperparameters, like `GridSearchCV`, however a fixed number of parameters are sampled, as defined by `n_iter` parameter.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od1RjPl504pw"
   },
   "outputs": [],
   "source": [
    "# import libraries - note we use scipy for generating a unifor distribution \n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# we can create hyperparameters as a list, as in type regularization penalty \n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# or as a distribution of values to sample from -'C' is the hyperparameter controlling the size of the regularisation penelty \n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "# we need to pass these parameters as a dictionary of {param_name: values}\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# we instantiate our model\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    "    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
    "    n_jobs=-1)\n",
    "\n",
    "# and fit it to the data \n",
    "best_model = randomizedsearch.fit(class_X, class_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z58WbRA-tDuL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.1696143431840764, solver='liblinear')"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can call this method to return the best parameters the search returned\n",
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2odbxKo040A"
   },
   "source": [
    "### EXERCISE FOR SECTION 6: \n",
    "* Part 1: Once we have fit the model to our data, we can call different methods on it, for example we can find out what the best parameters were, and we can predict using the best estimator returned by the search. Use the docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to call these methods on the `best_model` variable above. \n",
    "* Part 2: Implement `RandomSearchCV` on the Boston dataset\n",
    "  * Think about which parameters you want to specify\n",
    "  * You can pass these parameters to variables prior to creating the random search (as we do above)\n",
    "  * Then call methods as we do for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=3.1696143431840764, solver='liblinear')\n",
      "0.956078248719143\n",
      "{'C': 3.1696143431840764, 'penalty': 'l2'}\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EXERCISE SECTION 6: PART 1 \n",
    "  \n",
    "print(best_model.best_estimator_)\n",
    "print(best_model.best_score_)\n",
    "print(best_model.best_params_)\n",
    "print(best_model.best_index_)\n",
    "\n",
    "class_pred = best_model.best_estimator_.predict(class_X)\n",
    "class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emxT2zlrrzQ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  19.696199831814127\n",
      "Mean absolute error:  3.326904927834307\n",
      "R^2 :  0.7851038126487186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oandr\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:282: UserWarning: The total space of parameters 8 is smaller than n_iter=100. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 6: PART 2\n",
    "\n",
    "clf = Ridge(alpha=0.5)\n",
    "\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "\n",
    "hyperparameters = dict(alpha=alphas)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    clf, hyperparameters, random_state=1, n_iter=100, cv=4, verbose=0,\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Fit \n",
    "best_model_ = search.fit(dX_train, dy_train)\n",
    "\n",
    "# Predict\n",
    "pred_test_rnd = best_model_.best_estimator_.predict(dX_test)\n",
    "\n",
    "evaluate_reg(dy_test, pred_test_rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcFWj3HEgL9t"
   },
   "source": [
    "## 7. Ensemble models \n",
    "\n",
    "* The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. \n",
    "* There are several different approaches to achieve this, including **majority voting** ensemble methods, which we select the class label that has been predicted by the majority of classifiers.\n",
    "* The ensemble can be built from different classification algorithms, such as decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training set. \n",
    "* Indeed, Majority voting will work best if the classifiers used are different from each other and/or trained on different datasets (or subsets of the same data) in order for their errors to be uncorrelated.  Chapter 7 of *Python Machine Learning* has a good discussion of the principles behind Ensemble models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zCHjuFr1orF"
   },
   "outputs": [],
   "source": [
    "# import our classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# lets instantiate individual models \n",
    "log_clf = LogisticRegression(solver='liblinear')\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "# and an ensemble of them\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                              # here we select hard voting, which returns the majority of the predictions, not an average of probabilities\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blYK41CIoM-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9473684210526315\n",
      "RandomForestClassifier 0.9532163742690059\n",
      "SVC 0.9239766081871345\n",
      "VotingClassifier 0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "# here we can cycle through the individual estimators \n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    \n",
    "    # fit them to the training data \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # get a prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # and print the prediction \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIoSkRC31o36"
   },
   "source": [
    "### EXERCISE FOR SECTION 7: \n",
    "* Part 1: Implement **soft voting**.\n",
    " * If all the classifiers used in the ensemble are able to estimate class probabilities (they have a `predict_proba()` method that can be called on the estimator), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. \n",
    " *  Replace `voting=\"hard\"` with `voting=\"soft\"` and ensure that all your classifiers can estimate class probabilities. \n",
    "  * If you use the SVC estimator, you need to set `probability = True`, which will make the SVC class use cross-validation to estimate class probabilities and add a predict_proba() method. \n",
    "\n",
    "* Part 2: Implement Ensemble Learnng on the Boston Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KM8-EZ7S1pNA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9473684210526315\n",
      "RandomForestClassifier 0.9590643274853801\n",
      "SVC 0.9239766081871345\n",
      "VotingClassifier 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 7: PART 1 \n",
    "\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],voting='soft')\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    \n",
    "    # fit them to the training data \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # get a prediction\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    # and print the prediction \n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHtG2PNJpFFH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "Mean squared error:  23.91888317593273\n",
      "Mean absolute error:  3.516290110040999\n",
      "R^2 :  0.7390320547059954\n",
      "None\n",
      "Ridge\n",
      "Mean squared error:  19.42728523202775\n",
      "Mean absolute error:  3.2987471493270375\n",
      "R^2 :  0.7880378162997095\n",
      "None\n",
      "ElasticNet\n",
      "Mean squared error:  24.50940529586544\n",
      "Mean absolute error:  3.540721555469341\n",
      "R^2 :  0.7325891391586441\n",
      "None\n",
      "VotingRegressor\n",
      "Mean squared error:  22.230180788089797\n",
      "Mean absolute error:  3.414417904628196\n",
      "R^2 :  0.757456710620193\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE SECTION 7: PART 2 \n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "la_clf = Lasso(alpha=0.5)\n",
    "ri_clf = Ridge(alpha=0.5)\n",
    "el_clf = ElasticNet(alpha=0.5)\n",
    "\n",
    "vot_clf = VotingRegressor(estimators=[('la', la_clf), ('ri', ri_clf), ('el', el_clf)])\n",
    "\n",
    "for clf in (la_clf, ri_clf, el_clf, vot_clf):\n",
    "    \n",
    "    # fit them to the training data \n",
    "    clf.fit(dX_train, dy_train)\n",
    "    \n",
    "    # get a prediction\n",
    "    dy_pred = clf.predict(dX_test)\n",
    "    \n",
    "    # and print the prediction \n",
    "    print(clf.__class__.__name__)\n",
    "    print(evaluate_reg(dy_test, dy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojriAzfkgMXC"
   },
   "source": [
    "## 8. Building Pipelines\n",
    "* We can build a pipeline so that many of the steps we have covered to now can be implemented in one step, and be applied to the training and testing set without repeating code.\n",
    "* We will cover one implementation of a pipeline. The exercise below will link to further information for you to be able implement your own pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JR_8ZldC1rvI"
   },
   "outputs": [],
   "source": [
    "# import the libraries - these are dependent on what we wish to implement in our pipelines\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# this is the pipeline class required to implement all pipelines\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEmL5PNwslpH"
   },
   "outputs": [],
   "source": [
    "# here we instantiate the pipeline with our methods\n",
    "pipe_lr = make_pipeline(StandardScaler(),                    # takes an arbitary number of transfomers\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(random_state=1))  # but ends with an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhAe2-Wdslzd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=2)),\n",
       "                ('logisticregression', LogisticRegression(random_state=1))])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we fit our pipeline to the data, so that the steps above are executed on the training set\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2dROmyLvph8"
   },
   "outputs": [],
   "source": [
    "# here we call the predict method on our pipeline model against the test set \n",
    "y_pred = pipe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVyLf9QNvKzG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "# and print out the result\n",
    "print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wn3FnXs71r53"
   },
   "source": [
    "### EXERCISE FOR SECTION 8: \n",
    "* Implement a pipeline on the classification dataset (the one used in the example above). Specifically, implement `K-Fold Cross Validation` as set out in  *Python Machine Learning*, Chapter 6.\n",
    "  Moreinformation on pipelines is available at this [link](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html) blog series \n",
    "* Think about:\n",
    "  * the code being implemented for `K-Fold Cross Validation` as part of the pipeline. Put comments above the code to explain what is happening. \n",
    "  * how you want to preprocess the data\n",
    "  * whether you want to think about lower the dimensions of the data with something like `Principal Component Analysis`\n",
    "  * what algorithm you want to use \n",
    "  * make sure to predict the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.952\n",
      "Test Accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "## EXERCISE CODE HERE\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pipe = make_pipeline(MinMaxScaler(feature_range=(0, 1)), \n",
    "                        LogisticRegression(random_state=1)) \n",
    "\n",
    "cv_score = cross_val_score(pipe, X_train,y_train, cv=3)\n",
    "\n",
    "print('CV Accuracy: %.3f' % cv_score.mean()) \n",
    "\n",
    "y_pred = cross_val_predict(pipe, X_test,y_test, cv=3)\n",
    "\n",
    "print('Test Accuracy: %.3f' % accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7jIn9fwgMew"
   },
   "source": [
    "## 9. Review\n",
    "* We have covered:\n",
    "  * Scaling and Standardizing Data \n",
    "  * Model Evaluation and Metrics \n",
    "  * Classification and regression machine learning algorithms  \n",
    "  * Regularization\n",
    "  * Optimising parameters \n",
    "  * Ensemble models \n",
    "  * Building Pipelines\n",
    "* We have of course not covered everything that it is possible to cover, and there is plenty more to research. Please ask questions on Slack or follow-up with your own research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AP52AGypxJW5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "w0d_iE883eXP"
   ],
   "include_colab_link": true,
   "name": "Oxford_scikit_learn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
