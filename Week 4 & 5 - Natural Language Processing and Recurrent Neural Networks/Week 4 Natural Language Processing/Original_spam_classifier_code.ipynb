{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBpVIgYiBjJi"
   },
   "source": [
    "# Introduction\n",
    "A spam detector that will classify Short Message Service (SMS) or text messages as either \"ham\" or \"spam.\"\n",
    "\n",
    "The code available at: [GitHub](https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-2-and-Keras/blob/master/Chapter%207/spam_classifier.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uu0Pw8pB8xxS",
    "outputId": "bcfbab24-49da-48a2-ef5a-9ca486e97853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spam_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spam_classifier.py\n",
    "\n",
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def build_embedding_matrix(word2idx, embedding_dim, \n",
    "        embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:   # word not in embedding\n",
    "                pass\n",
    "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
    "            #     pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E\n",
    "\n",
    "\n",
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
    "            num_filters, kernel_sz, output_sz, \n",
    "            run_mode, embedding_weights, \n",
    "            **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz,\n",
    "            activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, \n",
    "            activation=\"softmax\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# data distribution is 4827 ham and 747 spam (total 5574), which \n",
    "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
    "# and this works out to being each spam (1) item as being approximately\n",
    "# 8 times as important as each ham (0) message.\n",
    "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--mode\", help=\"run mode\",\n",
    "    choices=[\n",
    "        \"scratch\",\n",
    "        \"vectorizer\",\n",
    "        \"finetuning\"\n",
    "    ])\n",
    "args = parser.parse_args()\n",
    "run_mode = args.mode\n",
    "\n",
    "# read data\n",
    "texts, labels = download_and_read(DATASET_URL)\n",
    "\n",
    "# tokenize and pad text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n",
    "\n",
    "# labels\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))\n",
    "\n",
    "# dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# embedding\n",
    "E = build_embedding_matrix(word2idx, EMBEDDING_DIM,\n",
    "    EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)\n",
    "\n",
    "# model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    run_mode, E)\n",
    "model.build(input_shape=(None, max_seqlen))\n",
    "model.summary()\n",
    "\n",
    "# compile and train\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# evaluate against test set\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ktVtIUJF9DyN",
    "outputId": "733ba293-6c02-48d4-ed99-0b290b3ce1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  datasets\tsample_data  spam_classifier.py\n",
      "mkdir: cannot create directory ‘data’: File exists\n",
      "data  datasets\tsample_data  spam_classifier.py\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "# Create data directory to store embeddings\n",
    "!mkdir data  \n",
    "# Verify the directory creation\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "4p9Jdnvy9HmB",
    "outputId": "76a4168a-0a00-43e2-bea5-13f3fe4e5eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 11:30:40.408929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "5574 sentences, max length: 189\n",
      "vocab size: 9010\n",
      "2020-05-29 11:30:42.039733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-05-29 11:30:42.041794: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2020-05-29 11:30:42.041837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (f0c1dcf8658c): /proc/driver/nvidia/version does not exist\n",
      "2020-05-29 11:30:42.048289: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n",
      "2020-05-29 11:30:42.048516: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2276d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-29 11:30:42.048561: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "Embedding matrix: (9010, 300)\n",
      "Model: \"spam_classifier_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2703000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  230656    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  514       \n",
      "=================================================================\n",
      "Total params: 2,934,170\n",
      "Trainable params: 231,170\n",
      "Non-trainable params: 2,703,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "29/29 [==============================] - 12s 422ms/step - loss: 0.6041 - accuracy: 0.8413 - val_loss: 0.1141 - val_accuracy: 0.9583\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 12s 418ms/step - loss: 0.2509 - accuracy: 0.9531 - val_loss: 0.0604 - val_accuracy: 0.9844\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 12s 415ms/step - loss: 0.1576 - accuracy: 0.9714 - val_loss: 0.1008 - val_accuracy: 0.9661\n",
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1091    0]\n",
      " [   0  189]]\n"
     ]
    }
   ],
   "source": [
    "!python spam_classifier.py --mode vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6D2wO2dB3CS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_bxt3J59SiP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Original_spam_classifier_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
